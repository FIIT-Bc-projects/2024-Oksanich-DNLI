{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:57:04.883384Z",
     "start_time": "2025-04-01T13:57:04.864871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from flwr.common import NDArrays\n",
    "import numpy as np\n",
    "\n",
    "# Simple vocabulary processor\n",
    "class VocabProcessor:\n",
    "    def __init__(self, max_vocab_size=10000, max_seq_length=100):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.word_counts = {}\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.vocab_size = 2  # Start with PAD and UNK tokens\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # Count all words\n",
    "        for text in texts:\n",
    "            for word in text.lower().split():\n",
    "                if word not in self.word_counts:\n",
    "                    self.word_counts[word] = 0\n",
    "                self.word_counts[word] += 1\n",
    "\n",
    "        # Select top words by frequency\n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for word, _ in sorted_words[:self.max_vocab_size - 2]:  # -2 for PAD and UNK\n",
    "            self.word2idx[word] = self.vocab_size\n",
    "            self.idx2word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.lower().split()[:self.max_seq_length]\n",
    "        indices = [self.word2idx.get(word, self.unk_idx) for word in tokens]\n",
    "\n",
    "        # Pad if necessary\n",
    "        if len(indices) < self.max_seq_length:\n",
    "            indices += [self.pad_idx] * (self.max_seq_length - len(indices))\n",
    "\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def get_attention_mask(self, encoded_text):\n",
    "        return (encoded_text != self.pad_idx).float()\n",
    "\n",
    "\n",
    "# Small RNN for sentiment analysis\n",
    "class SmallRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=64, num_classes=3, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "        # Initialize weights to limit model size\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, ids, attention_mask=None):\n",
    "        # ids shape: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(ids)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply mask to padding\n",
    "            embeddings = embeddings * attention_mask.unsqueeze(-1)\n",
    "\n",
    "        output, hidden = self.rnn(embeddings)  # output: (batch_size, seq_len, hidden_dim*2)\n",
    "\n",
    "        # Use the final hidden state from both directions\n",
    "        # Take mean across sequence length for a sequence representation\n",
    "        pooled = torch.mean(output, dim=1)  # (batch_size, hidden_dim*2)\n",
    "\n",
    "        dropped = self.dropout(pooled)\n",
    "        logits = self.fc(dropped)  # (batch_size, num_classes)\n",
    "\n",
    "        # Return logits and empty tuple to maintain compatibility with the transformer interface\n",
    "        return logits, ()\n",
    "\n",
    "\n",
    "# Utility functions for getting and setting weights (maintain compatibility)\n",
    "def get_weights(model):\n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "def set_weights(model, parameters):\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "    model.load_state_dict(state_dict, strict=True)\n"
   ],
   "id": "2c0b68222be0ae05",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:57:06.173712Z",
     "start_time": "2025-04-01T13:57:06.155077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "small_model = SmallRNN(\n",
    "    vocab_size=5000,\n",
    "    embedding_dim=32,\n",
    "    hidden_dim=32,\n",
    "    num_classes=3\n",
    ")\n",
    "sum(value.numel() for value in small_model.state_dict().values())"
   ],
   "id": "bd56b3509d64538c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T15:04:34.495225Z",
     "start_time": "2025-04-01T15:04:34.464096Z"
    }
   },
   "cell_type": "code",
   "source": "small_model.state_dict().keys()",
   "id": "eaa7c5d1026bc718",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding.weight', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'rnn.weight_ih_l0_reverse', 'rnn.weight_hh_l0_reverse', 'rnn.bias_ih_l0_reverse', 'rnn.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
