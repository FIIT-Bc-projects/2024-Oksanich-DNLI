{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:26.647949Z",
     "start_time": "2025-04-03T13:35:23.221977Z"
    }
   },
   "source": [
    "import torch as torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:26.673259Z",
     "start_time": "2025-04-03T13:35:26.660250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim, dropout=0.1):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_length, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(device)\n",
    "        embedding = self.word_embed(x) + self.pos_embed(positions)\n",
    "        return self.dropout(embedding)"
   ],
   "id": "a4a6fecc7d5ad893",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:27.132105Z",
     "start_time": "2025-04-03T13:35:27.117761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MHSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MHSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert (self.num_heads * self.head_dim == self.embed_dim), \\\n",
    "            'embed size must be divisible by number of heads'\n",
    "\n",
    "        self.w_queries = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.w_keys = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.w_values = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.head_dim * self.num_heads, self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape of x = [batch_size, sentence_length, embedding_dim]\n",
    "        batch_size = x.shape[0]\n",
    "        sentence_len = x.shape[1]\n",
    "\n",
    "        queries = self.w_queries(x).reshape(\n",
    "            batch_size, sentence_len, self.num_heads, self.head_dim).permute(\n",
    "            0, 2, 1, 3)\n",
    "\n",
    "        keys = self.w_keys(x).reshape(\n",
    "            batch_size, sentence_len, self.num_heads, self.head_dim).permute(\n",
    "            0, 2, 3, 1)\n",
    "\n",
    "        values = self.w_values(x).reshape(\n",
    "            batch_size, sentence_len, self.num_heads, self.head_dim).permute(\n",
    "            0, 2, 1, 3)\n",
    "\n",
    "        attention_scores = torch.einsum('bijk,bikl->bijl', queries, keys)\n",
    "        attention_dist = torch.softmax(attention_scores /\n",
    "                                   (self.embed_dim ** (1 / 2)), dim=-1)\n",
    "        attention_out = torch.einsum('bijk,bikl->bijl', attention_dist, values)\n",
    "        concatenated_out = attention_out.permute(0, 2, 1, 3).reshape(\n",
    "            batch_size, sentence_len, self.embed_dim)\n",
    "\n",
    "        return concatenated_out"
   ],
   "id": "da3ad248161ac846",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:30.265255Z",
     "start_time": "2025-04-03T13:35:30.254245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.attention = MHSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_out = self.dropout(self.attention(x))\n",
    "        x = self.norm1(x + attention_out)\n",
    "        forward_out = self.dropout(self.feed_forward(x))\n",
    "        out = self.norm2(x + forward_out)\n",
    "\n",
    "        return out"
   ],
   "id": "d686cb8f2b8ae6bd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:31.901565Z",
     "start_time": "2025-04-03T13:35:31.892089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim,\n",
    "                 num_heads, forward_expansion):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embedder = Embedding(vocab_size, max_length, embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim, num_heads, forward_expansion)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedder(x)\n",
    "        encoding = self.encoder(embedding)\n",
    "        compact_encoding = encoding.max(dim=1)[0]\n",
    "        out = self.fc(compact_encoding)\n",
    "        return out"
   ],
   "id": "7448a80934287756",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:35:32.780414Z",
     "start_time": "2025-04-03T13:35:32.706370Z"
    }
   },
   "cell_type": "code",
   "source": "classifier = Classifier(25000, 512, 128, 8, 3)",
   "id": "7813cc1fac0112d8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T13:36:08.403442Z",
     "start_time": "2025-04-03T13:36:08.394637Z"
    }
   },
   "cell_type": "code",
   "source": "sum(value.numel() for value in classifier.state_dict().values())",
   "id": "469bda12562f1000",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3430657"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
